{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Natural Langurage Processing (NLP) - \"Sentimental Analysis for Amazon Book Review\"\n",
    "# Name: Jianlei(John) Sun\n",
    "# Date: March 13, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/John/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import functools\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset from \"https://snap.stanford.edu/data/web-Amazon.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('./Kindle_Store_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/John/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df = df[['overall', 'reviewText']]\n",
    "df['overall'] = df[['overall']].apply(lambda x: 'pos' if x[0] >= 4.0 else 'neg', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  overall                                         reviewText\n",
       "0     pos  I enjoy vintage books and movies so I enjoyed ...\n",
       "1     pos  This book is a reissue of an old one; the auth...\n",
       "2     pos  This was a fairly interesting read.  It had ol...\n",
       "3     pos  I'd never read any of the Amy Brewster mysteri...\n",
       "4     pos  If you like period pieces - clothing, lingo, y..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('sampled_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file into DataFrame\n",
    "kindle_data = pd.read_csv('sampled_data.csv')\n",
    "type(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall    : pos\n",
      "reviewText : I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me.\n"
     ]
    }
   ],
   "source": [
    "# Print first row\n",
    "# Format: data_frame.col_nam[row]\n",
    "print(\"overall    :\", kindle_data.overall[0])\n",
    "print(\"reviewText :\", kindle_data.reviewText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>This book is a reissue of an old one; the auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>This was a fairly interesting read.  It had ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  overall                                         reviewText\n",
       "0     pos  I enjoy vintage books and movies so I enjoyed ...\n",
       "1     pos  This book is a reissue of an old one; the auth...\n",
       "2     pos  This was a fairly interesting read.  It had ol...\n",
       "3     pos  I'd never read any of the Amy Brewster mysteri...\n",
       "4     pos  If you like period pieces - clothing, lingo, y..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a sample (head) of the data frame\n",
    "kindle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    829277\n",
       "neg    153342\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statics on tags\n",
    "kindle_data.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitPosNeg(data_):\n",
    "    neg = data_.loc[data_.overall=='neg']\n",
    "    pos = data_.loc[data_.overall=='pos']\n",
    "    return [pos,neg]\n",
    "\n",
    "[pos,neg] = splitPosNeg(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "pos: 829277 , neg: 153342\n",
      "pos: 50000 , neg: 50000\n"
     ]
    }
   ],
   "source": [
    "print(type(pos))\n",
    "print(\"pos:\", len(pos), \", neg:\", len(neg))\n",
    "\n",
    "# sample small dataset\n",
    "pos = pos.head(50000)\n",
    "neg = neg.head(50000)\n",
    "print(\"pos:\", len(pos), \", neg:\", len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Yet a more compact way to write the code\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = str(line).translate(translation)  # Replace punctuation\n",
    "    line = nltk.word_tokenize(line.lower())  # Tokenize\n",
    "    \n",
    "    for t in line:\n",
    "        # Remove stopwords\n",
    "        if t not in stop:\n",
    "            stemmed = lemmatizer.lemmatize(t) # deal with nouns\n",
    "            stemmed = lemmatizer.lemmatize(stemmed, 'v') # deal with verbs\n",
    "            tokens.append(stemmed)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy movie yesterday really love'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"I bought those movies yesterday and I really loved them!\"\n",
    "preprocessing(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.60849404335022\n"
     ]
    }
   ],
   "source": [
    "# Yet a more modern way to write code\n",
    "start = time.time()\n",
    "\n",
    "pos_data = list(map(preprocessing, pos['reviewText']))\n",
    "neg_data = list(map(preprocessing, neg['reviewText']))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Data & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pos_data + neg_data\n",
    "# remember this is sampled\n",
    "labels = np.concatenate((pos['overall'].values,neg['overall'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set (20:80)\n",
    "# stratify: make sure pos/neg remains the same in training set and testing set\n",
    "train_data, test_data, train_labels, test_labels = \\\n",
    "train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size =  80000 testing size =  20000\n"
     ]
    }
   ],
   "source": [
    "print(\"training size = \", len(train_data), \"testing size = \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Push all tokens and compute frequency of words\n",
    "tokens = [word for line in train_data \\\n",
    "               for word in nltk.word_tokenize(line)]\n",
    "\n",
    "word_features = nltk.FreqDist(tokens)\n",
    "\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(10000))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf–idf term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tf: term-frequency\n",
    "- idf: inverse document-frequency\n",
    "- Tf-idf = $tf(t,d) \\times idf(t)$\n",
    "\n",
    "$$\n",
    "idf(t) = log{\\frac{1 + nd}{1 + df(d, t)}} + 1\n",
    "$$\n",
    "\n",
    "![](http://www.onemathematicalcat.org/Math/Algebra_II_obj/Graphics/log_base_gt1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9970 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9970 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since CountVectorizer and TfidTransformer are often used together\n",
    "# There is a class named TfidfVectorizer that combine these two steps\n",
    "tf_vec = TfidfVectorizer(min_df=1)\n",
    "tf_fit = tf_vec.fit_transform([' '.join(topwords)])\n",
    "tf_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features from training set\n",
    "# Vocabulary is from topwords\n",
    "train_features = tf_vec.transform(train_data)\n",
    "\n",
    "# cnt_train_features = cnt_vec.transform(train_data)\n",
    "# train_features = tf_trans.transform(cnt_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 9970)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array[n_train_data * n_features]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "test_features = tf_vec.transform(test_data)\n",
    "\n",
    "# cnt_test_features = cnt_vec.transform(test_data)\n",
    "# test_features = tf_trans.transform(cnt_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 19939)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Uni+Bi)-Gram\n",
    "bg_tf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "bg_tf_vec.fit([' '.join(topwords)])\n",
    "bg_train_features = bg_tf_vec.transform(train_data)\n",
    "\n",
    "bg_train_features.shape\n",
    "\n",
    "# Array[n_train_data * (uni_gram_features + bi_gram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract (uni+bi)-gram test features\n",
    "bg_test_features = bg_tf_vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing Multinomial NB\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for **classification with discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Uni-Gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB model trained in 0.209869 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "start = time.time()\n",
    "mnb_model.fit(train_features, train_labels)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Multinomial NB model trained in %f seconds\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'pos' 'pos' ..., 'pos' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79625\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "# mtrics.accuracy_score(y_true, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_true=test_labels, y_pred=pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.79      0.81      0.80     10000\n",
      "        pos       0.80      0.78      0.79     10000\n",
      "\n",
      "avg / total       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use keyword arguments to set arguments explicitly\n",
    "print(metrics.classification_report(y_true=test_labels, y_pred=pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Uni-Gram + Bi-Gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'pos' 'pos' ..., 'pos' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Train & test using (uni+bi)-gram features\n",
    "bg_mnb_model = MultinomialNB()\n",
    "bg_mnb_model.fit(bg_train_features, train_labels)\n",
    "bg_pred = bg_mnb_model.predict(bg_test_features)\n",
    "print(bg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7964\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "bg_accuracy = metrics.accuracy_score(bg_pred,test_labels)\n",
    "print(bg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.79      0.81      0.80     10000\n",
      "        pos       0.80      0.79      0.79     10000\n",
      "\n",
      "avg / total       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true=test_labels, y_pred=bg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a new sentence\n",
    "# vectorizer needs to be pre-fitted\n",
    "# The project function signature should be something like:\n",
    "# predict_new(sentent: str, vec, model) -> str\n",
    "\n",
    "def predict_new(sentence: str):\n",
    "    sentence = preprocessing(sentence)\n",
    "    features = tf_vec.transform([sentence])\n",
    "    pred = mnb_model.predict(features)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_new(\"I love it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save vectorizer\n",
    "with open('tf_vec.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(tf_vec, pkl_file)\n",
    "    \n",
    "# Save model\n",
    "with open('mnb_model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(mnb_model, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
